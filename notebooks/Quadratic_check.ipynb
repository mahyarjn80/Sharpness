{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting training...\n",
      "\n",
      "Epoch: 1/20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "derivative for aten::_scaled_dot_product_flash_attention_for_cpu_backward is not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 407\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 407\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_epoch(model, testloader, criterion, device)\n\u001b[1;32m    410\u001b[0m     epoch_train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[2], line 280\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, trainloader, criterion, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    279\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 280\u001b[0m     top_eigs \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_top_eigenvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     step_top_eigenvalues\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m: current_step,\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meigenvalues\u001b[39m\u001b[38;5;124m'\u001b[39m: top_eigs,\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    285\u001b[0m     })\n\u001b[1;32m    287\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[2], line 244\u001b[0m, in \u001b[0;36mcompute_top_eigenvalues\u001b[0;34m(model, inputs, labels, criterion, num_iterations)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Compute Hessian-vector product\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     Hv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(v)\n\u001b[0;32m--> 244\u001b[0m     grad_grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_vector\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     Hv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([gg\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m gg \u001b[38;5;129;01min\u001b[39;00m grad_grads])\n\u001b[1;32m    246\u001b[0m     v_new \u001b[38;5;241m=\u001b[39m Hv \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(Hv)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: derivative for aten::_scaled_dot_product_flash_attention_for_cpu_backward is not implemented"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate synthetic data\n",
    "n_samples = 200000\n",
    "n_features = 800\n",
    "\n",
    "# Create a diagonal covariance matrix with decreasing values\n",
    "# cov_diag = torch.linspace(10, 0.1, n_features)\n",
    "cov_diag = torch.zeros(n_features)\n",
    "cov_diag[0] = 10.0\n",
    "step_size = (n_features - 10) / (n_features - 1)\n",
    "for i in range(1, n_features):\n",
    "    cov_diag[i] = step_size\n",
    "cov_matrix = torch.diag(cov_diag)\n",
    "\n",
    "# Create a multivariate normal distribution with mean 0 and our covariance matrix\n",
    "mvn = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.zeros(n_features),\n",
    "    covariance_matrix=cov_matrix\n",
    ")\n",
    "\n",
    "# Generate random normal x by sampling from the multivariate normal distribution\n",
    "# This will give us a tensor of shape (n_samples, n_features)\n",
    "x_data = mvn.sample((n_samples,))\n",
    "\n",
    "# Generate a random unit vector u (the true parameter)\n",
    "u_true = torch.randn(n_features)\n",
    "u_true = u_true / torch.norm(u_true)\n",
    "\n",
    "# Generate y = x^T u + epsilon\n",
    "noise_level = 0.1\n",
    "epsilon = noise_level * torch.randn(n_samples)\n",
    "y_data = torch.matmul(x_data, u_true) + epsilon\n",
    "\n",
    "# Compute the covariance matrix of x_data\n",
    "x_cov = torch.matmul(x_data.T, x_data) / n_samples\n",
    "\n",
    "# Compute eigenvalues and eigenvectors of the covariance matrix\n",
    "eig_vals, eig_vecs = torch.linalg.eigh(x_cov)\n",
    "# Sort in descending order\n",
    "sorted_indices = torch.argsort(eig_vals, descending=True)\n",
    "eig_vals = eig_vals[sorted_indices]\n",
    "eig_vecs = eig_vecs[:, sorted_indices]\n",
    "\n",
    "# First eigenvector (corresponding to largest eigenvalue)\n",
    "first_eigenvector = eig_vecs[:, 1]\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return torch.mean((1/2) * (y_pred - y_true) ** 2)\n",
    "\n",
    "# Initialize model parameters randomly\n",
    "model = torch.nn.Linear(n_features, 1, bias=False)\n",
    "torch.nn.init.normal_(model.weight, mean=0.0, std=0.01)\n",
    "\n",
    "# SGD optimizer without momentum\n",
    "learning_rate = 2/(eig_vals[0]-0.0001)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "\n",
    "# Training loop - each step uses the full batch\n",
    "n_steps = 1000\n",
    "trajectory = []\n",
    "loss_values = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # Forward pass with full batch\n",
    "    y_pred = model(x_data).squeeze()\n",
    "    loss = loss_fn(y_pred, y_data)\n",
    "    \n",
    "    # Store loss\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    # Store projection of weights onto first eigenvector\n",
    "    w = model.weight.data.squeeze()\n",
    "    proj = torch.dot(w, first_eigenvector)\n",
    "    trajectory.append((proj.item(), loss.item()))\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f'Step [{step+1}/{n_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Convert trajectory to numpy for plotting\n",
    "trajectory = np.array(trajectory)\n",
    "\n",
    "# Plot the trajectory along the first eigenvector\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=2)\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], 'ro', alpha=0.5)\n",
    "plt.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10, label='Start')\n",
    "plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'mo', markersize=10, label='End')\n",
    "\n",
    "plt.xlabel('Projection onto First Eigenvector')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Optimization Trajectory along First Eigenvector Direction')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Also plot the loss curve over steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Step')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full script to calculate and validate eigenvalues and eigenvectors\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from scipy.sparse.linalg import LinearOperator, eigsh\n",
    "import time\n",
    "\n",
    "def compute_matrix_vector_product(matrix: Tensor, vector: Tensor):\n",
    "    \"\"\"Compute matrix-vector product for our randomly generated matrix.\"\"\"\n",
    "    # Ensure both tensors are on the same device\n",
    "    if matrix.device != vector.device:\n",
    "        vector = vector.to(matrix.device)\n",
    "    return matrix @ vector\n",
    "\n",
    "def lanczos(matrix: Tensor, neigs: int):\n",
    "    \"\"\" Invoke the Lanczos algorithm to compute the leading eigenvalues and eigenvectors of a matrix.\n",
    "    In this case, we directly use our randomly generated matrix rather than computing a Hessian. \"\"\"\n",
    "    \n",
    "    dim = matrix.shape[0]\n",
    "    device = matrix.device\n",
    "    \n",
    "    def mv(vec: np.ndarray):\n",
    "        gpu_vec = torch.tensor(vec, dtype=torch.float, device=device)\n",
    "        result = compute_matrix_vector_product(matrix, gpu_vec)\n",
    "        return result.cpu().numpy()\n",
    "\n",
    "    operator = LinearOperator((dim, dim), matvec=mv)\n",
    "    evals, evecs = eigsh(operator, neigs)\n",
    "    return torch.from_numpy(np.ascontiguousarray(evals[::-1]).copy()).float().to(device), \\\n",
    "           torch.from_numpy(np.ascontiguousarray(np.flip(evecs, -1)).copy()).float().to(device)\n",
    "\n",
    "def get_matrix_eigenvalues(matrix: Tensor, neigs=3):\n",
    "    \"\"\" Compute the leading eigenvalues and eigenvectors of our randomly generated matrix.\n",
    "    \n",
    "    Args:\n",
    "        matrix: The randomly generated matrix\n",
    "        neigs: Number of eigenvalues/eigenvectors to compute\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (eigenvalues, eigenvectors)\n",
    "    \"\"\"\n",
    "    evals, evecs = lanczos(matrix, neigs=neigs)\n",
    "    return evals, evecs\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Parameters\n",
    "n = 50000  # Matrix dimension\n",
    "rank = 50  # Rank of the low-rank matrix\n",
    "\n",
    "# Create a low-rank matrix\n",
    "print(f\"Creating a {n}x{n} low-rank matrix with rank {rank}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate a random matrix on CPU or GPU depending on availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create low-rank factors\n",
    "U = torch.randn(n, rank, device=device)\n",
    "matrix = U @ U.T  # This creates a symmetric positive semi-definite matrix\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "print(\"\\nCalculating top 3 eigenvalues and eigenvectors...\")\n",
    "eig_start_time = time.time()\n",
    "eigenvalues, eigenvectors = get_matrix_eigenvalues(matrix, neigs=3)\n",
    "eig_elapsed_time = time.time() - eig_start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Eigenvalue computation completed in {eig_elapsed_time:.2f} seconds\")\n",
    "print(\"\\nTop 3 eigenvalues:\")\n",
    "for i, val in enumerate(eigenvalues):\n",
    "    print(f\"λ{i+1} = {val:.6f}\")\n",
    "\n",
    "print(\"\\nEigenvector shapes:\", eigenvectors.shape)\n",
    "print(\"First few components of the first eigenvector:\", eigenvectors[:5, 0])\n",
    "\n",
    "# Validate if they are really eigenvalues and eigenvectors\n",
    "print(\"\\nValidating eigenvalues and eigenvectors...\")\n",
    "for i in range(3):\n",
    "    # For a valid eigenvector v with eigenvalue λ, we should have: Av ≈ λv\n",
    "    v = eigenvectors[:, i].reshape(-1, 1)\n",
    "    # Ensure v is on the same device as matrix\n",
    "    if v.device != matrix.device:\n",
    "        v = v.to(matrix.device)\n",
    "    Av = matrix @ v\n",
    "    lambda_v = eigenvalues[i] * v\n",
    "    \n",
    "    # Calculate the relative error\n",
    "    error = torch.norm(Av - lambda_v) / torch.norm(lambda_v)\n",
    "    \n",
    "    print(f\"Eigenvector {i+1} validation:\")\n",
    "    print(f\"  Relative error: {error.item():.6e}\")\n",
    "    print(f\"  Is valid eigenpair: {'Yes' if error < 1e-5 else 'No'}\")\n",
    "\n",
    "# Compare with torch.linalg.eigh for validation (only for small matrices)\n",
    "if n <= 2000:  # Only do this for reasonably sized matrices\n",
    "    print(\"\\nComparing with torch.linalg.eigh for validation...\")\n",
    "    # torch.linalg.eigh returns eigenvalues in ascending order\n",
    "    torch_eigenvalues, torch_eigenvectors = torch.linalg.eigh(matrix)\n",
    "    # Get the top 3 eigenvalues (largest magnitude)\n",
    "    top_indices = torch.argsort(torch_eigenvalues, descending=True)[:3]\n",
    "    torch_top_eigenvalues = torch_eigenvalues[top_indices]\n",
    "    \n",
    "    print(\"Top 3 eigenvalues from torch.linalg.eigh:\")\n",
    "    for i, val in enumerate(torch_top_eigenvalues):\n",
    "        print(f\"λ{i+1} = {val:.6f}\")\n",
    "    \n",
    "    # Calculate relative difference between our computed eigenvalues and torch's\n",
    "    # Ensure both tensors are on the same device\n",
    "    if eigenvalues.device != torch_top_eigenvalues.device:\n",
    "        eigenvalues_for_comparison = eigenvalues.to(torch_top_eigenvalues.device)\n",
    "    else:\n",
    "        eigenvalues_for_comparison = eigenvalues\n",
    "    rel_diff = torch.norm(eigenvalues_for_comparison - torch_top_eigenvalues) / torch.norm(torch_top_eigenvalues)\n",
    "    print(f\"Relative difference in eigenvalues: {rel_diff.item():.6e}\")\n",
    "    print(f\"Eigenvalues match: {'Yes' if rel_diff < 1e-5 else 'No'}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTotal computation completed in {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
